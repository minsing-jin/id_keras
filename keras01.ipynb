{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7667a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cd6f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "c = a+b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e56b87",
   "metadata": {},
   "source": [
    "<h1>넘파이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c56f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. 데이터준비\n",
    "# x,y -> 데이터 ->  최적의 weight와 최적의 bias를 구한다 -> 이걸통해서 최적의 output  미래 예측\n",
    "x = np.array([1,2,3])\n",
    "y = np.array([1,2,3])\n",
    "\n",
    "# 2. 인공신경망 모델을 구성한다\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 연속적으로 케라스 모델 연산하겠다\n",
    "from tensorflow.keras.layers import Dense                \n",
    "# dense layer에 연산을 하겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3895924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 3)                 6         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,196\n",
      "Trainable params: 1,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "# 판을 깔았다 -> 모델을 할 수 있는 판을 갈음\n",
    "\n",
    "model.add(Dense(3, input_dim=1)) \n",
    "\n",
    "# model에다가 dense layer를 붙이겠다\n",
    "# 한개의 모델을 구성하겠다\n",
    "\n",
    "\n",
    "model.add(Dense(4))\n",
    "model.add(Dense(2))\n",
    "model.add(Dense(1))\n",
    "# 레이어들을 추가하면서 노드의 개수를 정해줌, model.add -> 한개의 레이어 추가 -> Dense(3->노드의 개수) -> iㅜput dim -> input layer의 노드이 개수\n",
    "# 레이어를 순차적으로 쌓고 있음. input이 명시 안한이유 그 위에 단에 있는 곳이 순차적으로 쌓다보니 add된다\n",
    "# 3이 된다.\n",
    "# 2개의 인풋은 위에 4가 되는것임. 그 바로 위에 있는게 나의 인풋 , 나의 아래 레이어가 아웃풋레이어\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afd2f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0224\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 0.0078\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0151\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0086\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0057\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 986us/step - loss: 0.0061\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0075\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0097\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 981us/step - loss: 0.0042\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0050\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 989us/step - loss: 0.0034\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0043\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0050\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0025\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0052\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0038\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0030\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 996us/step - loss: 0.0017\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0024\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0023\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0028\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0013\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0014\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0015\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 8.9908e-04\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 8.3857e-04\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 498us/step - loss: 0.0016\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 4.7249e-04\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 996us/step - loss: 6.4141e-04\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 3.8739e-04\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 3.3270e-04\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 7.0611e-04\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3.6102e-04\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 3.4521e-04\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 984us/step - loss: 4.7711e-04\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 3.8389e-04\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 986us/step - loss: 2.0596e-04\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 4.0434e-04\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 1.5272e-04\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 3.0139e-04\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 996us/step - loss: 2.5339e-04\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 996us/step - loss: 2.5802e-04\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.2395e-04\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 498us/step - loss: 1.1492e-04\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 2.0278e-04\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 986us/step - loss: 1.7557e-04\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 6.7897e-05\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 8.1564e-05\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 9.6751e-05\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.2047e-04\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.0228e-04\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 9.0458e-05\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 7.6553e-05\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 984us/step - loss: 7.1290e-05\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 4.2141e-05\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.9715e-05\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 6.1841e-05\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.8583e-05\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 996us/step - loss: 3.9970e-05\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.7783e-05\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.7267e-05\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 2.0629e-05\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.5991e-05\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.6375e-05\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 1.2039e-05\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 2.5768e-05\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 1.9026e-05\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.2937e-05\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.4773e-05\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 7.6180e-06\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 7.7765e-06\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 6.5225e-06\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 9.6253e-06\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 8.3891e-06\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 6.4369e-06\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 4.1653e-06\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 4.1139e-06\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 498us/step - loss: 2.5064e-06\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 5.2011e-06\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 2.9991e-06\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 3.9647e-06\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 2.6364e-06\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 2.1015e-06\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 2.0740e-06\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 1.2848e-06\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 1.0594e-06\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.2344e-06\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 1.8123e-06\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 1.0379e-06\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 499us/step - loss: 1.0694e-06\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 7.9537e-07\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 6.9278e-07\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 1.1290e-06\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 1.3274e-06\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 997us/step - loss: 5.3433e-07\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 998us/step - loss: 4.6491e-07\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 998us/step - loss: 3.0225e-07\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000018232831160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 997us/step - loss: 5.6104e-07\n",
      "loss:  5.610403945865983e-07\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018231038A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "result:  [[3.998783  4.0036697 3.9980912 4.0011034 3.9987936 3.999478  4.000414\n",
      "  3.9976935 4.0000186 3.9989226]]\n"
     ]
    }
   ],
   "source": [
    "#model.compile(loss='mse', optimizer= 'adam')\n",
    "# 수치들과 모델의 간격은 mse로 설정, mes가 0.1이 나와있다\n",
    "# mse가 더 낮은 값이 나온다 -> 최적의 mes가 구해지고 있다\n",
    "# 저기서 loss와 optimizer은 하나의 파라미터들이다.\n",
    "# loss 최소 -1이므로 그걸 잡는거를 mse로 하겠다!\n",
    "# 이 모델에 대해서 로스와 옵티마이저 컴파일, 파이미터들을 정해줄거다\n",
    "# mes 알아오기\n",
    "# 최소의 로스를 만들기 위해서 adam이라는 옵티마이저를 쓸거다.\n",
    "\n",
    "# 3. 컴파일, 훈련\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(x,y, batch_size=1, epochs=100)  \n",
    "# 모델코딩하면 그림그릴수 있는지\n",
    "# 그림 보면 코딩할 수 있는지테스트\n",
    "# 숙제2 batch size 명시 안해준다면?? ---> 더 목표값에 덜 다가가게 됨 즉 loss가 더 커짐\n",
    "\n",
    "# 평가, 예측\n",
    "loss = model.evaluate(x,y,batch_size=1)\n",
    "print(\"loss: \", loss)\n",
    "\n",
    "result = model.predict([4])\n",
    "print(\"result: \", result)\n",
    "\n",
    "# 정제된 데이터\n",
    "# 최적의 weight,bias구함. -> loss가 가장 작아야함.-> 최적의 weight를 구함., 명시, 우린 mse를 기준으로 최소한의 loss를 구함. \n",
    "# 실제 데이터 간의 선을 그은것간의 최소값을 구해준 것임. 최적의 weight\n",
    "# 그렇게 하기 위해서는 x와 y를 넣어줌. 너에게 최적의 데이터를 주겠다. x와 y를 줄테니 머신은 최적의 W와 B를 구함. -> 우린 데이터를 정제해서 머신에게 집어넣어줌\n",
    "# 너는 이런식으로해서 모델을 구성해서 학습을해서 미래를 예측해라\n",
    "# 하이퍼 파리미터 값은 점점 가까워져야지, model,fit-> 훈련 많이 시킬수록 성능이 좋았음. 데이터 간의 간격을 구하기 위해서 epochs를 구하\n",
    "# 그 loss값을 구함. 선이 줄어들고 줄어들면 데이터가 주어지겠지, 훈련시킬 때마다 loss가 점점 줄어짐\n",
    "\n",
    "# 더좋아지다가 나빠지는것까지 봄, \n",
    "# 한마디로 훈련횟수가 많을수록 좋아지지만 너무 많이 훈련시키면 이상해짐 -> 즉 많은 공부를 할수록 손해가 날수도 있음\n",
    "# 우린 훈련한 데이터를 토대로  평가를 함.\n",
    "\n",
    "# 정제된데이터 준비 -> 모델구성 -> 컴파일, 훈련 -> 평가 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f7db2",
   "metadata": {},
   "source": [
    "깃허브 커밋할 수 있게 하기 레포지토리 하도록\n",
    "소스정리하는 것임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
